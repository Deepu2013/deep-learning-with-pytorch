{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:18.088250Z",
     "start_time": "2019-05-23T04:51:17.426066Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab1 \n",
    "Create a bag of word model for a text classification problem. Note that this is not the same as the continous bag of word problem that we solved here but you can reuse the tokenization part.\n",
    "\n",
    "https://github.com/yanneta/ML-notebooks/blob/master/cbow.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:21.661953Z",
     "start_time": "2019-05-23T04:51:21.658143Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "    ! mkdir data\n",
    "    ! tar -xvf rotten_imdb.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:22.437635Z",
     "start_time": "2019-05-23T04:51:22.287902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-14 11:42:27--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 519599 (507K) [application/x-gzip]\n",
      "Saving to: ‘rotten_imdb.tar.gz.1’\n",
      "\n",
      "rotten_imdb.tar.gz. 100%[===================>] 507.42K   557KB/s    in 0.9s    \n",
      "\n",
      "2020-05-14 11:42:29 (557 KB/s) - ‘rotten_imdb.tar.gz.1’ saved [519599/519599]\n",
      "\n",
      "mkdir: data: File exists\n",
      "x quote.tok.gt9.5000\n",
      "x plot.tok.gt9.5000\n",
      "x subjdata.README.1.0\n",
      "plot.tok.gt9.5000   quote.tok.gt9.5000  subjdata.README.1.0\n"
     ]
    }
   ],
   "source": [
    "get_data()\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:31.979291Z",
     "start_time": "2019-05-23T04:51:30.952129Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(\"data/quote.tok.gt9.5000\")\n",
    "obj_content = read_file(\"data/plot.tok.gt9.5000\")\n",
    "sub_content = np.array([line.strip().lower() for line in sub_content])\n",
    "obj_content = np.array([line.strip().lower() for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:51:31.992431Z",
     "start_time": "2019-05-23T04:51:31.982777Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (8000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"both lead performances are oscar-size . quaid is utterly fearless as the tortured husband living a painful lie , and moore wonderfully underplays the long-suffering heroine with an unflappable '50s dignity somewhere between jane wyman and june cleaver .\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a vocabulary\n",
    "* Split your sentences in tokens by spliting on spaces.\n",
    "* Compute the frequency of every word.\n",
    "* Pick top frequency words (4000 or so) to be part of your vocabulary.\n",
    "* Create a map from each word to an index. Keep 0 for out of the vocabulary workds (<UNK>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word representation\n",
    "\n",
    "* Given a piece of text compute the following features $x$.\n",
    "$x_i = 1$ if word with index $i$ appears in the text. Otherwise $x_i = 0$. Note that length $x$ is the size of the vocabulary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset and dataloaders\n",
    "Write a dataset for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:52:54.922573Z",
     "start_time": "2019-05-23T04:52:54.916113Z"
    }
   },
   "outputs": [],
   "source": [
    "class BOW(Dataset):\n",
    "    def __init__(self, ):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Define a simpler linear model or a two layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and valid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T04:56:29.431871Z",
     "start_time": "2019-05-23T04:56:29.426088Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_metrics(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    for x, y in valid_dl:\n",
    "        y_hat = model(x.float())\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y.float()).float().sum()\n",
    "        total += x.size(0)\n",
    "        loss_sum += loss.item()*x.size(0)\n",
    "    accuracy = correct.item()/total\n",
    "    return loss_sum/total, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:04:48.131508Z",
     "start_time": "2019-05-22T09:04:48.124575Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for x, y in train_dl:\n",
    "            y_hat = model(x.float())\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += x.size(0)\n",
    "            loss_sum += loss.item()*x.size(0)\n",
    "        val_loss, val_acc = val_metrics(model)\n",
    "        print(\"train loss %.3f val loss %.3f and accuracy %.3f\" % (loss_sum/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:05:03.287290Z",
     "start_time": "2019-05-22T09:04:54.494282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.656 val loss 0.615 and accuracy 0.875\n",
      "train loss 0.576 val loss 0.551 and accuracy 0.889\n",
      "train loss 0.512 val loss 0.501 and accuracy 0.893\n",
      "train loss 0.461 val loss 0.461 and accuracy 0.897\n",
      "train loss 0.422 val loss 0.429 and accuracy 0.898\n",
      "train loss 0.390 val loss 0.404 and accuracy 0.899\n",
      "train loss 0.364 val loss 0.383 and accuracy 0.898\n",
      "train loss 0.342 val loss 0.366 and accuracy 0.901\n",
      "train loss 0.324 val loss 0.352 and accuracy 0.902\n",
      "train loss 0.308 val loss 0.340 and accuracy 0.903\n",
      "train loss 0.294 val loss 0.329 and accuracy 0.904\n",
      "train loss 0.282 val loss 0.320 and accuracy 0.906\n",
      "train loss 0.271 val loss 0.312 and accuracy 0.907\n",
      "train loss 0.261 val loss 0.305 and accuracy 0.908\n",
      "train loss 0.253 val loss 0.299 and accuracy 0.910\n"
     ]
    }
   ],
   "source": [
    "model = BOWModel(vocab_size=len(vocab2index.keys()))\n",
    "train_epocs(model, 15, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance\n",
    "To get the words that affect the most the positive label we find the words with higher weights. Similarly to get the words that affect the most the 0 label we find the words with lower weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1818, -0.1209, -0.0691,  ..., -0.1578,  0.2273,  0.2485]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-0.0347], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms = [p for p in model.parameters()]\n",
    "parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18177307, -0.12088173, -0.06908546, ..., -0.15775996,\n",
       "         0.22728996,  0.24849562]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = parms[0].detach().numpy()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4009,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indeces = np.argsort(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.4535758, 0.44974813)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0, sorted_indeces[0]], weights[0, sorted_indeces[-1]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['material',\n",
       " 'performance',\n",
       " 'actors',\n",
       " 'movie',\n",
       " 'its',\n",
       " 'interesting',\n",
       " 'script',\n",
       " 'beautifully',\n",
       " 'movies',\n",
       " \"film's\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in sorted_indeces[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obsessed',\n",
       " 'kill',\n",
       " 'secret',\n",
       " 'school',\n",
       " 'patricia',\n",
       " 'sam',\n",
       " 'however',\n",
       " 'she',\n",
       " 'they',\n",
       " '-',\n",
       " 'discover']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in sorted_indeces[3998:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
