{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with feed forward Neural Network\n",
    "This notebook doesn't use GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install:\n",
    "* anaconda (Python 3)\n",
    "* conda install -c soumith pytorch \n",
    "* conda install -c soumith torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What is pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a popular framework for deep learning.\n",
    "\n",
    "PyTorch consists of 4 main packages:\n",
    "\n",
    "* torch: a general purpose array library similar to Numpy that can do computations on GPU when the tensor type is cast to (torch.cuda.TensorFloat)\n",
    "* torch.autograd: a package for building a computational graph and automatically obtaining gradients\n",
    "* torch.nn: a neural net library with common layers and cost functions\n",
    "* torch.optim: an optimization package with common optimization algorithms like SGD,Adam, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Here we load the dataset. In the future we will create our own datasets but MNIST dataset is part of Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('../data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('../data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset in Pytorch is a  subclass of ```torch.utils.data.Dataset```  thas has methods ```__getitem__``` and ```__len__``` methods implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has length\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can index any element\n",
    "# train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# train_ds[0] is a tuple with an image (x) and a class (y)\n",
    "x, y = train_ds[0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "Data loader combines a dataset and a sampler, and provides an iterator over the dataset. The data loader divides the data in mini batches. This is particularly important when working with large dataset that cannot be hold in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False) # for test we use shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)\n",
    "x, y = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, title=None):\n",
    "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first from torch to numpy\n",
    "X = x.numpy(); Y = y.numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X[0][0], Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn module\n",
    "a neural net library with common layers and cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear transformation of a Nx5 matrix into a Nx3 matrix, where N can be anything (number of observations)\n",
    "D = 5 # number of input featutes\n",
    "M = 3 # neurons in the first hidden layer\n",
    "linear_map = nn.Linear(D, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2469,  0.2184, -0.2992,  0.3448,  0.4097],\n",
       "         [-0.1085,  0.2023,  0.4396, -0.3135, -0.1455],\n",
       "         [-0.0978, -0.2650,  0.0308,  0.1142,  0.2913]]), Parameter containing:\n",
       " tensor([ 0.0017,  0.1238, -0.2413])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters are initialized randomly\n",
    "[p for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0926, -1.0000,  1.6144,  2.4546, -0.4632],\n",
       "        [-2.0533, -2.1369,  1.1997, -1.0973, -0.4270],\n",
       "        [-0.2235,  0.4419, -1.7092,  1.6288, -0.6967],\n",
       "        [ 2.5760,  1.0105, -0.3007, -2.8154, -0.8766],\n",
       "        [ 0.6501, -0.8153,  0.8810,  2.5546, -0.4635],\n",
       "        [-0.7481,  1.2996,  0.3010,  0.1027,  2.0781],\n",
       "        [-0.0145, -1.1060,  0.3728,  2.0404, -1.7696],\n",
       "        [-0.5944,  0.2549, -0.5292, -0.4740, -0.4061],\n",
       "        [-0.3065, -0.9603,  0.9908,  1.6842,  0.7530],\n",
       "        [ 2.4662,  0.6887, -1.5430,  0.4662,  0.2807]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random tensor\n",
    "# think about A as a dataset with 10 points and 5 features\n",
    "# the linear map with transform \n",
    "N = 10 \n",
    "A = torch.randn(N, D) \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0660, -0.0608,  0.2280],\n",
       "        [-1.8845,  0.8478,  0.3131],\n",
       "        [ 0.8307, -0.9232, -0.4062],\n",
       "        [-0.3815,  0.9267, -1.3472],\n",
       "        [ 0.4115, -0.4577,  0.0952],\n",
       "        [ 0.8977,  0.2656,  0.1139],\n",
       "        [-0.3765, -0.3166, -0.2177],\n",
       "        [-0.2609,  0.2149, -0.4395],\n",
       "        [ 0.3091, -0.2392,  0.4855],\n",
       "        [ 1.4986, -0.8698, -0.5775]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_map(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating models \n",
    "There are two ways of define a model in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "To create a model with nn.Sequential you provide a list of layers. For example, the following model defines a 2-layer neural network with 784 input features ($D = 784$), 300 hidden layers ($M=300$) and 10 outputs. This model uses Relu activation funtion and no final activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(784, 300),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(300, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module\n",
    "A more flexible way to define models in pytorch is as a subclass of nn.Module. In the ```__init__``` method we define all layers that will be used later. In the forward method, we define the actual model using the already defined layers. Here is the same example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables (self).\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 300)\n",
    "        self.linear2 = nn.Linear(300, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line prints all parameters\n",
    "#[p for p in net.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "```torch.optim``` provides implementations of commonly used optimization algorithms (such us gradient descent and momentum). You need to specify the algorithm you want to use. Adam is a popular algorithm. You also specify the parameters you want to optimize and the learning rate. If you want to use $L_2$ regularization you can specify the weight decay. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is one training epoch \n",
    "def train_model(train_loader, model, optimizer):\n",
    "   \n",
    "    model.train() # set model to training mode\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):   # looping through training observations\n",
    "        batch = images.shape[0] # size of the batch\n",
    "        images = images.view(-1, 28*28) # fattening the images\n",
    "    \n",
    "        pred = model(images) # prediction\n",
    "        # Computing loss. Note that F.cross_entropy combines log_softmax and\n",
    "        # nll_loss in a single function. That is why there is no softmax at the end of our model.\n",
    "        loss = F.cross_entropy(pred, labels) \n",
    "        \n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Makes an update to the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += batch\n",
    "        sum_loss += batch * loss.item()\n",
    "                \n",
    "    train_loss = sum_loss/total\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Given and model and a data loader we compute loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, data_loader):\n",
    "    model.eval() # set model to evaluation mode\n",
    "    correct = 0   \n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for images, labels in data_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        sum_loss += labels.size(0)*loss.item()\n",
    "        total += labels.size(0)\n",
    "        _, pred = torch.max(outputs.data, 1) # computes a hard prediction\n",
    "        correct += pred.eq(labels.data).sum().item()\n",
    "    return 100 * correct / total, sum_loss/ total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss  0.2819532489339511\n",
      "accuracy and test loss  93.9 0.2116187918946147\n",
      "train loss  0.209114771267275\n",
      "accuracy and test loss  94.78 0.222731652796641\n",
      "train loss  0.18917332437112927\n",
      "accuracy and test loss  94.25 0.2497051699474454\n",
      "train loss  0.1868301212174197\n",
      "accuracy and test loss  94.21 0.27098039969690146\n",
      "train loss  0.17512419914230704\n",
      "accuracy and test loss  94.92 0.19857247819900511\n",
      "train loss  0.1716309383617093\n",
      "accuracy and test loss  93.93 0.3154376177391037\n",
      "train loss  0.16212673492059113\n",
      "accuracy and test loss  94.18 0.2565289742290974\n",
      "train loss  0.1562506328223894\n",
      "accuracy and test loss  95.59 0.22041507729638835\n",
      "train loss  0.15083103455857685\n",
      "accuracy and test loss  95.06 0.21142924510817976\n",
      "train loss  0.15740459418514122\n",
      "accuracy and test loss  95.89 0.24820291886106133\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    train_loss = train_model(train_loader, net, optimizer)\n",
    "    print(\"train loss \", train_loss)\n",
    "    acc, loss = model_eval(net, test_loader)\n",
    "    print(\"accuracy and test loss \", acc, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "* Extend the model to a 3-layer neural network.\n",
    "* Modify the training by starting with 0.01 learning rate for 5 epochs and then lowering learning rate to 0.001 for another 5 epochs.\n",
    "* Write a function that computes F1 score.\n",
    "* Change the optimizer to train with gradient descent with momemtum (instead of Adam). \n",
    "* How much can you increase the batch size and be able to train in your laptop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-optim\n",
    "* https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 0.4 version of Pytorch had a few changes.\n",
    "https://pytorch.org/2018/04/22/0_4_0-migration-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
